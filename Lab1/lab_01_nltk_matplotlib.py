# -*- coding: utf-8 -*-
"""Lab_01_NLTK_Matplotlib.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1bAbjYVmfyDBF2ziDJfSqE-E7-ltYc70I
"""

import nltk
import matplotlib.pyplot as plt
import pandas as pd

my_text = "#github was down for approx a min, who noticed this??"

import re
import string
from nltk.corpus import stopwords
from nltk.stem import PorterStemmer
from nltk.tokenize import TweetTokenizer

remove_link = re.sub(r'https?:\/\/.*[\r\n]*', '', my_text)
remove_link = re.sub(r'#', '', remove_link)
print(remove_link)

from nltk.tokenize import sent_tokenize
text="Daniel James leaves United for Â£25m. Only Cristiano Ronaldo, Romelu Lukaku and Angel di Maria have left the club for higher fees #mufc"
# download punkt
nltk.download("punkt")
tokenized_text=sent_tokenize(text)
print(tokenized_text)

from nltk.tokenize import word_tokenize
tokenized_word=word_tokenize(text)
print(tokenized_word)

# frequency distribution
from nltk.probability import FreqDist
fdist = FreqDist(tokenized_word)
fdist.most_common(4)
# Frequency Distribution Plot
import matplotlib.pyplot as plt
fdist.plot(30, cumulative = False, color = "green")
plt.show()

# stop words
from nltk.corpus import stopwords
# download stopwords
nltk.download("stopwords")
stop_words = set(stopwords.words("english"))
print(stop_words)

filtered_sent=[]
for w in tokenized_word:
    if w not in stop_words:
        filtered_sent.append(w)
print("Tokenized Sentence:",tokenized_word)
print("Filterd Sentence:",filtered_sent)

# stemming
from nltk.stem import PorterStemmer
from nltk.tokenize import sent_tokenize, word_tokenize

ps = PorterStemmer()

stemmed_words=[]
for w in filtered_sent:
    stemmed_words.append(ps.stem(w))

print("Filtered Sentence:",filtered_sent)
print("Stemmed Sentence:",stemmed_words)

#Lexicon Normalization
#performing stemming and Lemmatization

from nltk.stem.wordnet import WordNetLemmatizer
nltk.download('wordnet')
lem = WordNetLemmatizer()

from nltk.stem.porter import PorterStemmer
stem = PorterStemmer()

word = "crying"
print("Lemmatized Word:",lem.lemmatize(word, "v"))
print("Stemmed Word:",stem.stem(word))